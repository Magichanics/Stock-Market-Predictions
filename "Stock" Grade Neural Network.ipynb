{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# \"Stock\" Grade Neural Network\n### Starter Kernel by ``Magichanics`` \n*([GitHub](https://github.com/Magichanics) - [Kaggle](https://www.kaggle.com/magichanics))*\n\nWith more features from public kernels, as well as the idea of using Neural Networks for modelling, I've decided to do some experimenting myself in hopes of producing the best results. Feel free to post suggestions or criticisms!"
    },
    {
      "metadata": {
        "_uuid": "c6e9c58ee0984fd7aace750a5f542ceee09e17f0"
      },
      "cell_type": "markdown",
      "source": "## Table of Contents\n\n* [Step 1. Merging Datasets](#section1)\n* [Step 2. Feature Engineering](#section2)\n* [Step 3. Modelling using Keras' Neural Network](#section3)\n* [Step 4. Applying the Model](#section4)"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom itertools import chain\n\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9242e2e05ad6ae6897c99a33e224cc588621028",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# import environment for data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "451e7c9bd35702323a7df4db34b91921254a1369"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\nmarket_train_df = market_train_df.tail(400_000)\nnews_train_df = news_train_df.tail(1_000_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a58dabdb861587a6dc5d2917ecc742727f3d1e60"
      },
      "cell_type": "markdown",
      "source": "<a id='section1'></a>\n## Step 1. Merging Datasets\n\nWhile most of the notebooks focuses only on the market dataset, I'm going to attempt on bringing both the news and market dataset together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98ddae667a406f38302133fe776106f15e873eca",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3573a5f315d0fad58e8102d3a80d1f8cb269f8b7"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f41abc107bdef775535477c924c4ff591d298fbf"
      },
      "cell_type": "markdown",
      "source": "### Time difference between time and firstCreated\nMaybe the news isn't that urgent if there was a time difference between the two columns."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db40e27033e7a0c25103c32c83d1e54223ee8459"
      },
      "cell_type": "code",
      "source": "# WIP, will work on it later on\ndef time_diff(news_df):\n    news_train_df['num_publishing_diff_secs'] = news_train_df['time'] - news_train_df['firstCreated']\n    news_train_df['num_publishing_diff_secs'] = news_train_df['num_publishing_diff_secs'] / np.timedelta64(1, 's')\n    return news_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5532e82bb00bea0fee62a7b8fd1fc226f2fc348c"
      },
      "cell_type": "code",
      "source": "news_train_df = time_diff(news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13f7b88ce7c3473340251903b17106af89e8e213"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data\nWe will be removing the rows with the following qualities:\n* Empty headlines\n* Repeat headlines\n* Urgency of 2\n* Null assetName"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2a9a6a69a66557f69b30b5e27553a2ba15bc933"
      },
      "cell_type": "code",
      "source": "def clean_data(market_df, news_df, train=True):\n    \n    # get rid of invalid rows\n    news_df = news_df[news_df.headline != '']\n    news_df = news_df[news_df.urgency != 2]\n    \n    # remove duplicate headlines with the same assetCodes\n    news_df = news_df.drop_duplicates(subset=['assetCodes', 'headline'],keep='first')\n    \n#     if train:\n#         market_df.drop('assetName', axis=1, inplace=True)\n\n    return market_df, news_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e814659d584d93fa399c07746d050ba5ca62b2e"
      },
      "cell_type": "code",
      "source": "market_train_df, news_train_df = clean_data(market_train_df, news_train_df, train=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54ea9c0ee8287b5edc984a84c07030a0e106d6c8"
      },
      "cell_type": "code",
      "source": "news_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4b177c09e0546499a260041b7fb604673813ccf"
      },
      "cell_type": "code",
      "source": "market_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "486d593869589819aefadb4345af1b5db0060ffc"
      },
      "cell_type": "markdown",
      "source": "### Expanding News data\nWe are going to be splitting the news data by assetCode."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29b5aa276e3bfb61ae5a83dbb24162da1f0a72c6"
      },
      "cell_type": "code",
      "source": "def expanding_news(news_df):\n    \n    # split to list\n    news_output = news_df.copy()\n    news_output['assetCodes'] = news_output['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n    \n    # separate to assetcodes\n    assetCodes_expanded = list(chain(*news_output['assetCodes']))\n    assetCodes_index = news_df.index.repeat(news_output['assetCodes'].apply(len))\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    # merge to dataframe\n    merging_cols = [f for f in news_output if f not in ['assetCodes', 'sourceId']]\n    news_df_expanded = pd.merge(df_assetCodes, news_output[merging_cols], left_on='level_0', \n                                right_index=True, suffixes=(['','_old']))\n    \n    return news_df_expanded",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "783cae5ad82f345e1d620fcdf7b22e9c76fa3e29"
      },
      "cell_type": "code",
      "source": "expand_train_df = expanding_news(news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "368a6fd592fc50eeefa583123cb1b1fa7ad28dd3"
      },
      "cell_type": "code",
      "source": "expand_train_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e4b182a9fd045db6a0ef4941324471ef60496f6"
      },
      "cell_type": "code",
      "source": "market_train_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e2c2e63e365f4b89f660f17ffbeb38545a0c05db"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Headlines\nThe following will simplify strings to only get the necessary words needed for text processing."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8e89a5e3cfd230ee137cf88be5f557765243d5e"
      },
      "cell_type": "code",
      "source": "from nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\nps = PorterStemmer()\nsw = stopwords.words('english')\n\n# this takes up a lot of time, so apply it when getting coefficients to filter out words.\ndef clean_headlines(headline):\n    \n    # remove numerical and convert to lowercase\n    headline =  re.sub('[^a-zA-Z]',' ',headline)\n    headline = headline.lower()\n    \n    # use stemming to simplify words\n    headline_words_rough = headline.split(' ')\n    \n    # check if stopwords are present in headlines\n    headline_words = []\n    for word in headline_words_rough:\n        if word not in sw:\n            # use stemming to simplify\n            headline_words.append(ps.stem(word))\n    \n    # join sentence back again\n    return ' '.join(headline_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c90a1d3d5da39a17a36b06a756470ff6dee6d53b"
      },
      "cell_type": "markdown",
      "source": "### Categorical Groupby\nThis will merge groups of categorical data together into either lists or sets."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27a4e721de05191399c01e8724a39022d0f1b77e"
      },
      "cell_type": "code",
      "source": "def categorical_groupby(expand_df):\n\n    # get categorical groupbys\n    main_cols = ['time', 'assetCode']\n    expand_headline_groupby = expand_df[main_cols + ['headline']].groupby(['time', 'assetCode'])\n    expand_cat_groupby = expand_df[main_cols + ['subjects', 'audiences']].groupby(['time', 'assetCode'])\n    \n    # split subjects and audiences\n    def cat_to_list(x):\n        if x.name not in ['time', 'assetCode'] and x.name != 'headline':\n            result = []\n            for item in x:\n                result += item\n            return result\n        elif x.name == 'headline':\n            return list(x)\n    \n    # convert groupby to dataframes\n    expand_cat_df = expand_cat_groupby.transform(lambda x: cat_to_list(x))\n    expand_headline_df = expand_headline_groupby.transform(lambda x: cat_to_list(x)) # can't iterate through?\n\n    # merge to categorical dataframes\n    return pd.concat([expand_cat_df, expand_headline_df], axis=1)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0f32a0c7e865a495e93cb7d0144aadfa5eddda2"
      },
      "cell_type": "markdown",
      "source": "### Numerical Groupby\nThis will merge groups of numerical data together through aggregating the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02bbfecae766db4df6b63c24b49c2c294f9fab96"
      },
      "cell_type": "code",
      "source": "def numerical_groupby(expand_df):\n    \n    # get aggregated columns + aggregation map\n    news_agg_cols = [f for f in news_train_df.columns if 'novelty' in f or\n                    'volume' in f or\n                    'sentiment' in f or\n                    'bodySize' in f or\n                    'Count' in f or\n                    'marketCommentary' in f or\n                    'relevance' in f or\n                    'num_' in f]\n    news_agg_dict = {}\n    for col in news_agg_cols:\n        news_agg_dict[col] = ['mean', 'sum', 'max', 'min']\n    news_agg_dict['urgency'] = ['min', 'count']\n    news_agg_dict['takeSequence'] = ['max']\n    \n    # aggregate dataframe\n    expand_agg_groupby = expand_df[['time', 'assetCode'] + sorted(list(news_agg_dict.keys()))].groupby(['time', 'assetCode'])\n    expand_agg_df = expand_agg_groupby.agg(news_agg_dict).apply(np.float32)\n    expand_agg_df.columns = ['_'.join(col).strip() for col in expand_agg_df.columns.values]\n    \n    return expand_agg_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5538d778f93b8193ec36ef8c11d6ac0ea3f305a0"
      },
      "cell_type": "markdown",
      "source": "### Merge by time &  assetCode to News Article\nWe will be merging rows with the same time and assetCode."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1653c7e20b8d9bcd4541c2f7da29549d3b1f7f2b"
      },
      "cell_type": "code",
      "source": "def get_matches(market_df, expand_df):\n    \n    # get temporary columns as data\n    temp_market_df = market_df[['time', 'assetCode']].copy()\n    temp_expand_df = expand_df[['time', 'assetCode']].copy()\n    \n    # get indecies\n    temp_expand_df['expand_index'] = temp_expand_df.index.values\n    \n    # join the two\n    temp_expand_df.set_index(['time', 'assetCode'], inplace=True)\n    temp_expand_market_df = temp_market_df.join(temp_expand_df, on=['time', 'assetCode'])\n    \n    # remove nulls\n    temp_expand_market_df = temp_expand_market_df[temp_expand_market_df.expand_index.isnull() == False]\n    expand_indicies = temp_expand_market_df['expand_index'].tolist()\n    \n    # do final cleanup\n    del temp_market_df\n    del temp_expand_df\n    \n    # fetch matches\n    return expand_df.loc[expand_indicies]\n\ndef merge_by_code(market_df, expand_df):\n    \n    # get expansion of rows\n    expand_df = get_matches(market_df, expand_df)\n    \n    # prepare categorical features for merging\n    expand_df['subjects'] = expand_df['subjects'].str.findall(f\"'([\\w\\./]+)'\")\n    expand_df['audiences'] = expand_df['audiences'].str.findall(f\"'([\\w\\./]+)'\")\n    expand_df['headline'] = expand_df['headline'].apply(clean_headlines)\n    \n    # groupby datasets\n    expand_cat_df = categorical_groupby(expand_df)\n    expand_num_df = numerical_groupby(expand_df)\n    \n    # merge datasets\n    expanded_market_df = market_df.join(expand_num_df, on=['time', 'assetCode'])\n    expanded_market_df = expanded_market_df.join(expand_cat_df) # m.index.values in n.index.values >>False\n    \n    return expanded_market_df\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6429fe3977ddb8aaeaf32dd91ee566fdf1d10fd"
      },
      "cell_type": "code",
      "source": "m = merge_by_code(market_train_df, expand_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0510497535f716d6d841fa4e5057283812a53d35"
      },
      "cell_type": "code",
      "source": "n = merge_by_code(market_train_df, expand_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "705ea159e53099bcb6c7d1c14ab480612bb9cdf3"
      },
      "cell_type": "code",
      "source": "m.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddbb804bb0b7d1c13c4f09c0592945dcf1148e25"
      },
      "cell_type": "code",
      "source": "m.index.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f46896abf80e4ce517c7dc7667be1331be2d7148"
      },
      "cell_type": "code",
      "source": "n.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6c2dbd7501c063d2bcedace705066c53a90fd81"
      },
      "cell_type": "code",
      "source": "n.index.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63f79d63d3a8cdd415782f29a6c972fb181222d0"
      },
      "cell_type": "code",
      "source": "X_train[X_train.headline.isnull() == False].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e40ace1c0615dacdfc9bb59f81a163f7f8795c65"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b24a1eb7424ef10af9dbb4ee7c177dfefa220f51"
      },
      "cell_type": "markdown",
      "source": "<a id='section2'></a>\n## Step 2. Feature Engineering\n\nFrom Quant features to text processing features."
    },
    {
      "metadata": {
        "_uuid": "f505dbf3b51058742233233511a2a6173c6d558f"
      },
      "cell_type": "markdown",
      "source": "### News Features\n* Last News Article - This feature will have the number of days it has been since a news article has targeted the given assetCode\n* Number of Articles Today/Week/Month - Fetches the number of Articles that was written on the assetCode during the given timeframe."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e15e09b098a0272bf34512be2198fab2bce25ca"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d1321bf02dbbcb9564eba994d75d0b4821f23bf"
      },
      "cell_type": "markdown",
      "source": "### Entire Market and Individual Asset Quant Features\nWe are going to be obtaining Quant Features from both the entire market dataframe and from each individual asset based on assetCode."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5364a8a9a634f55570c0ee68ade6830d3ba84350"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26fb740a713963d533d1cb9684b81a3f6795525a"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with CountVectorizer and TfidfVectorizer\nWe are going to be using CountVectorizer and TfidfVectorizer on the headlines to determine its influence on the target column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90878febb77aa9c2a14f420fedc3a8763e019e4e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee95784fb1993f7f76453efc7714a23fb6e83f9a"
      },
      "cell_type": "markdown",
      "source": "### Clustering\nWe will be clustering the open and close features using KMeans."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85c1e43b0e3d2b082a3e2badb0c13da2f3b0562c"
      },
      "cell_type": "code",
      "source": "def clustering(X):\n\n    def cluster_modelling(features):\n        df_set = X[features]\n        cluster_model = KMeans(n_clusters = 8)\n        cluster_model.fit(df_set)\n        return cluster_model.predict(df_set)\n    \n    # get columns:\n    vol_cols = [f for f in X.columns if f != 'volume' and 'volume' in f]\n    novelty_cols = [f for f in X.columns if 'novelty' in f]\n    \n    # fill nulls\n    cluster_cols = novelty_cols + vol_cols + ['open', 'close']\n    X[cluster_cols] = X[cluster_cols].fillna(0)\n    \n    X['cluster_open_close'] = cluster_modelling(['open', 'close'])\n    X['cluster_volume'] = cluster_modelling(vol_cols)\n    X['cluster_novelty'] = cluster_modelling(novelty_cols)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84d6bbc9c805b3581afd36b49f80b583b1f9a5dc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab3199b41454b6093f0cd2be4dddf3b869824596"
      },
      "cell_type": "markdown",
      "source": "<a id='section3'></a>\n## Step 3. Modelling using Keras' Neural Network"
    },
    {
      "metadata": {
        "_uuid": "5853a3b60388bbab1fcd48a0d98d446d0f3d5826"
      },
      "cell_type": "markdown",
      "source": "### Preparing Datasets for Modelling\nWe will convert all the numerical and categorical datasets into rows that the neural network can process."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7230685136b783d05cd1069f3c680ec2120d6485"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\n# scale numerical columns\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(market_train_df[test_cols].fillna(0))\n\ny_train = market_train_df['returnsOpenNextMktres10']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4401ce84c431dc13bedb4e00b9b8f7d97145f7c3"
      },
      "cell_type": "code",
      "source": "def get_cols(X_train):\n    \n    # get numerical and categorical columns\n    num_cols = [f for f in X_train.columns if X_train[f].dtype == 'int' or X_train[f].dtype == 'float' and f not in ['universe', 'returnsOpenNextMktres10']]\n    cat_cols = [f for f in X_train.columns if f not in num_cols and f not in ['universe', 'returnsOpenNextMktres10']]\n    \n    return num_cols, cat_cols",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "76abdc01a2846fe429c2364a2a21cecae3e92fc0"
      },
      "cell_type": "markdown",
      "source": "### Fixed Training Split\nThe reason why we need to do a fixed training test split that fetches the last few rows of the training dataset is to avoid odd results, since randomly choosing rows will cause the validation dataset to be filled with rows with different timestamps."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96cba104e412b54f84ec6f348c633df92577c90c"
      },
      "cell_type": "code",
      "source": "def fixed_train_test_split(X, y, train_size):\n    \n    # round train size\n    train_size = int(train_size * len(X))\n    \n    # split data\n    X_train, y_train = X[train_size:], y[train_size:]\n    X_valid, y_valid = X[:train_size], y[:train_size]\n    \n    return X_train, y_train, X_valid, y_valid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f554061d572cb8f8fd06cef48c3ef633e9db17d0"
      },
      "cell_type": "code",
      "source": "X_train, y_train, X_valid, y_valid = fixed_train_test_split(X_train, y_train, 5000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77a9aa17efdcf7cdad38590f92691b6e09761d67"
      },
      "cell_type": "code",
      "source": "# original from https://www.kaggle.com/christofhenkel/market-data-nn-baseline\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy\n\n# categorical data\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n    \ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\n# numerical data\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\n# combined\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(128,activation='relu')(logits)\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd66639fbbfdeb0a3f0fffd349bf0ea523c1edfa"
      },
      "cell_type": "code",
      "source": "get_cols(market_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c8dad53c38c7c24c88ec93d7ce8cf0c06807ad3"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\n# set cylical learning rate per epoch\nlearning_rate = 1e-4\ndynamic_lr = LearningRateScheduler(lambda epoch: learning_rate * 0.99 ** epoch)\n\n# set early stopping\nearly_stop = EarlyStopping(patience=3)\n\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=200,\n          verbose=0,\n         callbacks=[dynamic_lr, early_stop]) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4c3c68f4745f6dc58181e8bc8e5a552d2385677"
      },
      "cell_type": "code",
      "source": "model.predict(X_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36a59163f089651745eaf40bb6250261a23aab24"
      },
      "cell_type": "markdown",
      "source": "<a id='section4'></a>\n## Step 4. Applying the Model\n"
    },
    {
      "metadata": {
        "_uuid": "cbcb1733a22212e40026b5bafdbb982b667446d6"
      },
      "cell_type": "markdown",
      "source": "**Sources:**\n* [Market Data NN Baseline by Christofhenkel](https://www.kaggle.com/christofhenkel/market-data-nn-baseline)\n* [a simple model using the market and news data by Bguberfain](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data)\n* [Amateur Hour - Using Headlines to Predict Stocks by Magichanics](https://www.kaggle.com/magichanics/amateur-hour-using-headlines-to-predict-stocks)\n* [Simple Quant Features by Youhanlee](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
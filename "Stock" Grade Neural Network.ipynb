{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# \"Stock\" Grade Neural Network\n### Starter Kernel by ``Magichanics`` \n*([GitHub](https://github.com/Magichanics) - [Kaggle](https://www.kaggle.com/magichanics))*\n\nWith more features from public kernels, as well as the idea of using Neural Networks for modelling, I've decided to do some experimenting myself in hopes of producing the best results. Feel free to post suggestions or criticisms!"
    },
    {
      "metadata": {
        "_uuid": "c6e9c58ee0984fd7aace750a5f542ceee09e17f0"
      },
      "cell_type": "markdown",
      "source": "## Table of Contents\n\n* [Step 1. Merging Datasets](#section1)\n* [Step 2. Feature Engineering](#section2)\n* [Step 3. Modelling using Keras' Neural Network](#section3)\n* [Step 4. Applying the Model](#section4)"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom itertools import chain\n\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9242e2e05ad6ae6897c99a33e224cc588621028"
      },
      "cell_type": "code",
      "source": "# import environment for data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "451e7c9bd35702323a7df4db34b91921254a1369"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\nmarket_train_df = market_train_df.tail(40_000)\nnews_train_df = news_train_df.tail(100_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a58dabdb861587a6dc5d2917ecc742727f3d1e60"
      },
      "cell_type": "markdown",
      "source": "<a id='section1'></a>\n## Step 1. Merging Datasets\n\nWhile most of the notebooks focuses only on the market dataset, I'm going to attempt on bringing both the news and market dataset together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98ddae667a406f38302133fe776106f15e873eca"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3573a5f315d0fad58e8102d3a80d1f8cb269f8b7"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13f7b88ce7c3473340251903b17106af89e8e213"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data\nSome of the rows in the dataset does not make sense, so we will be removing said rows."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2a9a6a69a66557f69b30b5e27553a2ba15bc933"
      },
      "cell_type": "code",
      "source": "def clean_data(market_df, news_df):\n    \n    # get rid of blank headlines\n    news_df = news_df[news_df.headline != '']\n    \n    return market_df, news_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e814659d584d93fa399c07746d050ba5ca62b2e"
      },
      "cell_type": "code",
      "source": "market_train_df, news_train_df = clean_data(market_train_df, news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "486d593869589819aefadb4345af1b5db0060ffc"
      },
      "cell_type": "markdown",
      "source": "### Expanding News data\nWe are going to be splitting the news data by assetCode."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29b5aa276e3bfb61ae5a83dbb24162da1f0a72c6"
      },
      "cell_type": "code",
      "source": "def expanding_news(news_df):\n    \n    # split to list\n    news_output = news_df.copy()\n    news_output['assetCodes'] = news_output['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n    \n    # separate to assetcodes\n    assetCodes_expanded = list(chain(*news_output['assetCodes']))\n    assetCodes_index = news_df.index.repeat(news_output['assetCodes'].apply(len))\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    # merge to dataframe\n    merging_cols = [f for f in news_output if f not in ['assetCodes', 'sourceId']]\n    news_df_expanded = pd.merge(df_assetCodes, news_output[merging_cols], left_on='level_0', \n                                right_index=True, suffixes=(['','_old']))\n    \n    return news_df_expanded",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "783cae5ad82f345e1d620fcdf7b22e9c76fa3e29"
      },
      "cell_type": "code",
      "source": "temp_expand_df = expanding_news(news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "368a6fd592fc50eeefa583123cb1b1fa7ad28dd3"
      },
      "cell_type": "code",
      "source": "temp_expand_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e2c2e63e365f4b89f660f17ffbeb38545a0c05db"
      },
      "cell_type": "markdown",
      "source": "### Merge by Closest Time to News Article\nWe will be adding each news article to their closest market row in terms of time, but with no news article being over one hour."
    },
    {
      "metadata": {
        "_uuid": "b24a1eb7424ef10af9dbb4ee7c177dfefa220f51"
      },
      "cell_type": "markdown",
      "source": "<a id='section2'></a>\n## Step 2. Feature Engineering\n\nFrom Quant features to text processing features."
    },
    {
      "metadata": {
        "_uuid": "f505dbf3b51058742233233511a2a6173c6d558f"
      },
      "cell_type": "markdown",
      "source": "### News Features\n* Last News Article - This feature will have the number of days it has been since a news article has targeted the given assetCode\n* Number of Articles Today/Week/Month - Fetches the number of Articles that was written on the assetCode during the given timeframe."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e15e09b098a0272bf34512be2198fab2bce25ca"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d1321bf02dbbcb9564eba994d75d0b4821f23bf"
      },
      "cell_type": "markdown",
      "source": "### Entire Market and Individual Asset Quant Features\nWe are going to be obtaining Quant Features from both the entire market dataframe and from each individual asset based on assetCode."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5364a8a9a634f55570c0ee68ade6830d3ba84350"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26fb740a713963d533d1cb9684b81a3f6795525a"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with CountVectorizer and TfidfVectorizer\nWe are going to be using CountVectorizer and TfidfVectorizer on the headlines to determine its influence on the target column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90878febb77aa9c2a14f420fedc3a8763e019e4e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee95784fb1993f7f76453efc7714a23fb6e83f9a"
      },
      "cell_type": "markdown",
      "source": "### Clustering\nWe will be clustering the open and close features using KMeans."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85c1e43b0e3d2b082a3e2badb0c13da2f3b0562c"
      },
      "cell_type": "code",
      "source": "def clustering(X):\n\n    def cluster_modelling(features):\n        df_set = X[features]\n        cluster_model = KMeans(n_clusters = 8)\n        cluster_model.fit(df_set)\n        return cluster_model.predict(df_set)\n    \n    # get columns:\n    vol_cols = [f for f in X.columns if f != 'volume' and 'volume' in f]\n    novelty_cols = [f for f in X.columns if 'novelty' in f]\n    \n    # fill nulls\n    cluster_cols = novelty_cols + vol_cols + ['open', 'close']\n    X[cluster_cols] = X[cluster_cols].fillna(0)\n    \n    X['cluster_open_close'] = cluster_modelling(['open', 'close'])\n    X['cluster_volume'] = cluster_modelling(vol_cols)\n    X['cluster_novelty'] = cluster_modelling(novelty_cols)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84d6bbc9c805b3581afd36b49f80b583b1f9a5dc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab3199b41454b6093f0cd2be4dddf3b869824596"
      },
      "cell_type": "markdown",
      "source": "<a id='section3'></a>\n## Step 3. Modelling using Keras' Neural Network"
    },
    {
      "metadata": {
        "_uuid": "5853a3b60388bbab1fcd48a0d98d446d0f3d5826"
      },
      "cell_type": "markdown",
      "source": "### Preparing Datasets for Modelling\nWe will convert all the numerical and categorical datasets into rows that the neural network can process."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7230685136b783d05cd1069f3c680ec2120d6485"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\n# scale numerical columns\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(market_train_df[test_cols].fillna(0))\n\ny_train = market_train_df['returnsOpenNextMktres10']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4401ce84c431dc13bedb4e00b9b8f7d97145f7c3"
      },
      "cell_type": "code",
      "source": "def get_cols(X_train):\n    \n    # get numerical and categorical columns\n    num_cols = [f for f in X_train.columns if X_train[f].dtype == 'int' or X_train[f].dtype == 'float' and f not in ['universe', 'returnsOpenNextMktres10']]\n    cat_cols = [f for f in X_train.columns if f not in num_cols and f not in ['universe', 'returnsOpenNextMktres10']]\n    \n    return num_cols, cat_cols",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "76abdc01a2846fe429c2364a2a21cecae3e92fc0"
      },
      "cell_type": "markdown",
      "source": "### Fixed Training Split\nThe reason why we need to do a fixed training test split that fetches the last few rows of the training dataset is to avoid odd results, since randomly choosing rows will cause the validation dataset to be filled with rows with different timestamps."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96cba104e412b54f84ec6f348c633df92577c90c"
      },
      "cell_type": "code",
      "source": "def fixed_train_test_split(X, y, train_size):\n    \n    # round train size\n    train_size = int(train_size * len(X))\n    \n    # split data\n    X_train, y_train = X[train_size:], y[train_size:]\n    X_valid, y_valid = X[:train_size], y[:train_size]\n    \n    return X_train, y_train, X_valid, y_valid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f554061d572cb8f8fd06cef48c3ef633e9db17d0"
      },
      "cell_type": "code",
      "source": "X_train, y_train, X_valid, y_valid = fixed_train_test_split(X_train, y_train, 5000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77a9aa17efdcf7cdad38590f92691b6e09761d67"
      },
      "cell_type": "code",
      "source": "# original from https://www.kaggle.com/christofhenkel/market-data-nn-baseline\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy\n\n# categorical data\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n    \ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\n# numerical data\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\n# combined\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(128,activation='relu')(logits)\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd66639fbbfdeb0a3f0fffd349bf0ea523c1edfa"
      },
      "cell_type": "code",
      "source": "get_cols(market_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c8dad53c38c7c24c88ec93d7ce8cf0c06807ad3"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\n# set cylical learning rate per epoch\nlearning_rate = 1e-4\ndynamic_lr = LearningRateScheduler(lambda epoch: learning_rate * 0.99 ** epoch)\n\n# set early stopping\nearly_stop = EarlyStopping(patience=3)\n\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=200,\n          verbose=0,\n         callbacks=[dynamic_lr, early_stop]) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4c3c68f4745f6dc58181e8bc8e5a552d2385677"
      },
      "cell_type": "code",
      "source": "model.predict(X_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36a59163f089651745eaf40bb6250261a23aab24"
      },
      "cell_type": "markdown",
      "source": "<a id='section4'></a>\n## Step 4. Applying the Model\n"
    },
    {
      "metadata": {
        "_uuid": "cbcb1733a22212e40026b5bafdbb982b667446d6"
      },
      "cell_type": "markdown",
      "source": "**Sources:**\n* [Market Data NN Baseline by Christofhenkel](https://www.kaggle.com/christofhenkel/market-data-nn-baseline)\n* [a simple model using the market and news data by Bguberfain](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data)\n* [Amateur Hour - Using Headlines to Predict Stocks by Magichanics](https://www.kaggle.com/magichanics/amateur-hour-using-headlines-to-predict-stocks)\n* [Simple Quant Features by Youhanlee](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
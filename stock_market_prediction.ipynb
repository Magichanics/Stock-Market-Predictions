{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2db4ea9ce6bed60c3ffa0cf5ff09b0d4efbb589"
      },
      "cell_type": "markdown",
      "source": "# Stock Market Prediction - Starter Kernel\n### Created by Magichanics\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74ac86edc14f5f49372114111d24ba7f9c095d97"
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom itertools import chain\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_squared_error # wouldn't recommend since we're not being evaluated on MSE\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport gc",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7033afd72b1b362694154c7955485b171d52c4d"
      },
      "cell_type": "markdown",
      "source": "### Importing Dataframes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5b9954e4a0dac7c2efe89d0bdb14b0d50ae1886",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading the data... This could take a minute.\nDone!\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aabe093aa9598bd90bd7bec571df3e8cbdfb8d99"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8191b24f88a743876d8554490924d904581aa081"
      },
      "cell_type": "code",
      "source": "# decide the length of the dataset\n# note if sampling, len(news_train_df) > len(market_train_df)\nmarket_train_df = market_train_df.tail(100_000)\nnews_train_df = news_train_df.tail(300_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07732b5c4888f5de18aa4c424267715dbed5783f"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "997773dbe4987fbec07c28775c4343acda7f9521"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb841bc6ab0199143c83428d5419ba9a28eec13b"
      },
      "cell_type": "markdown",
      "source": "### Information on the Training Data\n* There are no Unknown ``assetName`` in ``news_train_df``, but there are 24 479 rows with Unknown as the ``assetName`` in ``market_train_df``. Merging by ``assetCode`` leaves out Unknown rows, which could be problematic.\n* ``Volume`` has the highest correlation in terms of ``returnsOpenNextMktres10``\n* Merging by just ``assetCodes`` greatly increases the dataframe (with just 100k rows, it has turned into 10 million rows), although merging by ``assetCodes`` and ``time`` greatly decrease the original dataframe."
    },
    {
      "metadata": {
        "_uuid": "c1185cd336552dfbc3972772b6cc1ad908654b43"
      },
      "cell_type": "markdown",
      "source": "### Joining Market & News Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce1b56bfcb10695990c379ec9c7c44b1b04a1b2a"
      },
      "cell_type": "code",
      "source": "#tempcode\n# decide the length of the dataset\n# note if sampling, len(news_train_df) > len(market_train_df)\n(market_train_df, news_train_df) = env.get_training_data()\nmarket_train_df = market_train_df.tail(100_000)\nnews_train_df = news_train_df.tail(300_000)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2dad49559a057808e6d2559a6fbac4f964bd6d4"
      },
      "cell_type": "code",
      "source": "def join_market_news(market_df, news_df):\n    \n    # ERROR: REDUCES LENGTH OF DATAFRAME HERE\n    print('market_df :' + str(market_df.shape))\n    \n    # Fix asset codes (str -> list)\n    news_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_df['assetCodes']))\n    assetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    # create dataframe based on groupby\n    news_df_expanded = pd.merge(df_assetCodes, news_df, left_on='level_0', right_index=True, suffixes=(['','_old']))\n    \n#     # create dict of values based on columns\n#     str_cols = ['time', 'assetCode', 'headline']\n#     news_data_dict = {}\n#     news_groupby = news_df_expanded.groupby(['time', 'assetCode'])\n#     for f in str_cols:\n#         news_data_dict[f] = news_groupby[f].apply(lambda x: x)\n#         print('finished ' + f)\n    \n#     news_df_grouped = pd.DataFrame(news_data_dict)\n#     #return news_df_grouped\n    \n    #X = market_train_df.join(news_df_grouped, on=['time', 'assetCode'])\n    #X = pd.merge(news_df_grouped, market_df, on=['time', 'assetCode'])\n    print('merging data')\n    \n    # store unknown rows in a separate dataframe.\n    market_unknown = market_df[market_df.assetName == 'Unknown']\n    \n    # merge both dataframes\n    X = pd.merge(news_df_expanded, market_df, on=['assetCode'])\n    X = pd.concat([X, market_unknown], axis=1, sort=False)\n    \n    # cleanup time\n    del market_df\n    del news_df\n    gc.collect()\n    \n    print('X shape :' + str(X.shape))\n    \n    return X\n\nX_train = join_market_news(market_train_df, news_train_df)\n\n# headlines                                          wordCount\n# 0     ((2016-12-29 11:30:02+00:00, GPT), [GRAMERCY P...           ((2016-12-29 11:30:02+00:00, GPT), [15])\n# 1     ((2016-12-29 11:30:02+00:00, GPT.N), [GRAMERCY...         ((2016-12-29 11:30:02+00:00, GPT.N), [15])\n# 2     ((2016-12-29 11:30:07+00:00, GPT), [GRAMERCY P...           ((2016-12-29 11:30:07+00:00, GPT), [10])\n# 3     ((2016-12-29 11:30:07+00:00, GPT.N), [GRAMERCY...         ((2016-12-29 11:30:07+00:00, GPT.N), [10])\n# 4     ((2016-12-29 11:30:49+00:00, HYG.TO), [HYDROGE...         ((2016-12-29 11:30:49+00:00, HYG.TO), [9])\n# 5     ((2016-12-29 11:30:49+00:00, HYGS.O), [HYDROGE...         ((2016-12-29 11:30:49+00:00, HYGS.O), [9])\n# 6     ((2016-12-29 11:30:49+00:00, HYGS.OQ), [HYDROG...        ((2016-12-29 11:30:49+00:00, HYGS.OQ), [9])",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "market_df :(100000, 16)\nmerging data\nX shape :(10925926, 68)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ea2d11216ec83dd96df7e999d4e73a8b6427d636"
      },
      "cell_type": "markdown",
      "source": "### Aggregations"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c12d655c50595d0a1a1efad04c4e5d315ad7136"
      },
      "cell_type": "code",
      "source": "def aggregations(df):\n    \n    # get columns\n    lst_of_lst_of_cols = [[f for f in df.columns if 'return' in f and f != 'returnsOpenNextMktres10'],\n                              [f for f in df.columns if 'novelty' in f],\n                              [f for f in df.columns if 'volume' in f],\n                              [f for f in df.columns if 'sentiment' in f]]\n    \n    agg_suffixes = ['aggReturn ', 'aggNovelty ', 'aggVolume ', 'aggSentiment ']\n    \n    for i_cols in range(len(lst_of_lst_of_cols)):\n        \n        # setup map of aggregations\n        agg_dict = {}\n        for col in lst_of_lst_of_cols[i_cols]:\n            agg_dict[col] = ['mean', 'var', 'sum', 'std', 'max', 'min']\n            \n        # preform aggregations\n        df_agg = df.groupby('sourceId').agg(agg_dict)\n        df_agg.columns = pd.Index(['agg_' + e[0] + \"_\" + e[1].lower() for e in df_agg.columns.tolist()])\n        \n        # clean up dataframe and merge\n        df = df.join(df_agg, how = 'left', on = 'sourceId', lsuffix = agg_suffixes[i_cols])\n        del df_agg\n        gc.collect()\n        \n        print('finished ' + agg_suffixes[i_cols])\n        \n    print('New dataframe shape: ' + str(df.shape))\n    return df\n\nX_train = aggregations(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40c1624897798c163daa2f0cdb15e9dbf8c7ba63"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with MultinomialNB (Headlines)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e5984f2f62e60adc9e6561eb8d2e1bc1076224a"
      },
      "cell_type": "code",
      "source": "def get_headline(headlines_df):\n    \n    # get headlines as list\n    headlines_lst = []\n    for row in range(0,len(headlines_df.index)):\n        headlines_lst.append(headlines_df.iloc[row])\n\n    # split headlines to separate words\n    basicvectorizer = CountVectorizer()\n    headlines_vectorized = basicvectorizer.fit_transform(headlines_lst)\n    \n    print(headlines_vectorized.shape)\n    return headlines_vectorized, basicvectorizer\n\ndef headline_mapping(target, headlines_vectored, headline_vectorizer):\n    \n    # round target values if using logistic regression\n    target = round(target,0)\n    \n    # get model (testing with model that isn't )\n    from sklearn.naive_bayes import MultinomialNB\n    headline_model = MultinomialNB()\n    headline_model = headline_model.fit(headlines_vectored, target)\n    \n    # get coefficients\n    basicwords = headline_vectorizer.get_feature_names()\n    basiccoeffs = headline_model.coef_.tolist()[0]\n    coeff_df = pd.DataFrame({'Word' : basicwords, \n                            'Coefficient' : basiccoeffs})\n    \n    # convert dataframe to dictionary of coefficients\n    coefficient_dict = dict(zip(coeff_df.Word, coeff_df.Coefficient))\n\n    return coefficient_dict, coeff_df['Coefficient'].mean()\n\ndef get_coeff_col(headlines_df, coeff_dict, coeff_default):\n    \n    def get_coeff(word_lst):\n        \n        # iter through every word\n        coeff_sum = 0\n        for word in word_lst:\n            if word in coeff_dict:\n                coeff_sum += coeff_dict[word]\n            else:\n                coeff_sum += coeff_default\n        \n        # get average coefficient\n        return coeff_sum / len(word_lst)\n        \n    basicvectorizer = CountVectorizer()\n    \n    # loop through every item\n    headlines_coeff_lst = []\n    for row in range(0,len(headlines_df.index)):\n        headlines_coeff_lst.append(get_coeff(str(headlines_df.iloc[row]).split(' ')))\n    \n    return pd.Series(headlines_coeff_lst)\n\ncoefficient_dict, coefficient_default = headline_mapping(X_train['returnsOpenNextMktres10'],\n                                            *get_headline(X_train['headline']))\n\nX_train['headline_coeff_mean'] = get_coeff_col(X_train['headline'], coefficient_dict, coefficient_default)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc940f982e945d6e76161896290e004ee6850486"
      },
      "cell_type": "code",
      "source": "X_train['headline_coeff_mean'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "957f29e020695f32628882848eecdf392aa9e9e0"
      },
      "cell_type": "markdown",
      "source": "### Merge Dataframes"
    },
    {
      "metadata": {
        "_uuid": "b6bc63a91be4bd007fb95c728aadd14a870358ce"
      },
      "cell_type": "markdown",
      "source": "### Get Time Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83c73101057e4aa8c72784ab3fb3c3b497677fc1"
      },
      "cell_type": "code",
      "source": "# ripped from my previous kernel, NYC Taxi Fare\n\n# first get dates\ndef split_time(df):\n    \n    # convert to string (will find a more efficient way to do this without converting to string)\n    df['time'] = df['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    \n    # split date_time into categories\n    df['time_day'] = df['time'].str.slice(8,10)\n    df['time_month'] = df['time'].str.slice(5,7)\n    df['time_year'] = df['time'].str.slice(0,4)\n    df['time_hour'] = df['time'].str.slice(11,13)\n    \n    # source: https://www.kaggle.com/nicapotato/taxi-rides-time-analysis-and-oof-lgbm\n    df['temp_time'] = df['time'].str.replace(\" UTC\", \"\")\n    df['temp_time'] = pd.to_datetime(df['temp_time'], format='%Y-%m-%d %H:%M:%S')\n    \n    df['time_day_of_year'] = df.temp_time.dt.dayofyear\n    df['time_week_of_year'] = df.temp_time.dt.weekofyear\n    df[\"time_weekday\"] = df.temp_time.dt.weekday\n    df[\"time_quarter\"] = df.temp_time.dt.quarter\n    \n    del df['temp_time']\n    gc.collect()\n    \n    # convert to non-object columns\n    time_feats = ['time_day', 'time_month', 'time_year', 'time_hour']\n    df[time_feats] = df[time_feats].apply(pd.to_numeric)\n    \n    # determine whether the day is set on a holiday\n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start='2007-01-01', end='2018-09-27').to_pydatetime()\n    df['on_holiday'] = df['time'].str.slice(0,10).apply(lambda x: 1 if x in holidays else 0)\n    \n    # note to self: encode time later on\n    \n    return df\n\nX_train = split_time(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f855dca36dec6cd621f032991abc7beb711022e"
      },
      "cell_type": "code",
      "source": "def get_misc_features(X_df):\n    \n    # Adding daily difference\n    new_col = X_df[\"close\"] - X_df[\"open\"]\n    X_df.insert(loc=6, column=\"daily_diff\", value=new_col)\n    X_df['close_to_open'] =  np.abs(X_df['close'] / X_df['open'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3c97e050f19db860fa531455d45aabea3ec9322"
      },
      "cell_type": "markdown",
      "source": "### Label Encoding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e08ff349608e1ee591566ccdbfcaf78c0180661"
      },
      "cell_type": "code",
      "source": "def group_delete(df, del_features):\n    for f in del_features:\n        del df[f]\n\ndef encoding(df, categorical_feats):\n    df_encoded = pd.get_dummies(df[categorical_feats])\n    df.join(df_encoded, how = 'right')\n    group_delete(df, categorical_feats)\n    print('new shape: ' + str(df.shape))\n    return df\n\ngroup_delete(X_train, ['time', 'sourceId', 'headline', 'assetCodes'])\nX_train = encoding(X_train, [f for f in X_train.columns if X_train[f].dtype == 'object'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cc55c845df528961c590be653077ad6418586e2"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a68828c7def0ec9db579d76c280fb433218acf8b"
      },
      "cell_type": "code",
      "source": "# will use a more efficient way later on\nfcol = [c for c in X_train.columns if c not in ['sourceTimestamp', 'firstCreated', 'returnsOpenNextMktres10', \n                                                'assetName_x', 'universe', 'provider', 'subjects',\n                                               'audiences', 'marketCommentary', 'assetName_y', 'sourceTimestamp'\n                                               'firstCreated']] #<---- added\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07a88fc50ca5727ed3f299ab2a9ffd4b2ae05e33"
      },
      "cell_type": "markdown",
      "source": "### Using LGBM for Modelling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2c866081e0fc66578711db888176ee183f26ec3"
      },
      "cell_type": "code",
      "source": "# prepare x dataframes for modelling/prediction\ndef convert_to_X(market_obs_df, news_obs_df):\n    \n    # this repeats everything that was done previously\n    X_test = join_market_news(market_obs_df, news_obs_df)\n    X_test = aggregations(X_test)\n    X_test['headline_coeff_mean'] = get_coeff_col(X_test['headline'], coefficient_dict, coefficient_default)\n    X_test = split_time(X_test)\n    group_delete(X_test, ['time', 'sourceId', 'headline', 'assetCodes'])\n    X_test = encoding(X_test, ['assetCode', 'headlineTag'])\n    X_test = X_test[[f for f in X_test.columns if 'int' in str(X_test[f].dtype) or 'float' in str(X_test[f].dtype)]]\n    \n    return X_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b70209ab5a7928aa810fb6a9efb1b71fd5698e86"
      },
      "cell_type": "code",
      "source": "y_train = X_train['returnsOpenNextMktres10']\ndel X_train['returnsOpenNextMktres10']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d5b1e3030ee1e87c65689f99260f74f63f23fdb"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\nimport time\n\n# set model and parameters\nparams = {'learning_rate': 0.02, \n          'boosting': 'gbdt', \n          'objective': 'regression', \n          'seed': 2018}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c9c43854bc92e13e6808160edd5725e56790cd6"
      },
      "cell_type": "code",
      "source": "#split data (for cross validation)\nx1, x2, y1, y2 = train_test_split(X_train[fcol], \n                                  y_train, \n                                  test_size=0.25, \n                                  random_state=99)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6929a08a5e902a56e29b8b1fcb49d8f6c7e0fcc2"
      },
      "cell_type": "code",
      "source": "# train\nt = time.time()\nprint('Fitting Up')\n\n# cross validation\nlgb_model = lgb.train(params, \n                        lgb.Dataset(x1, label=y1), \n                        5000, \n                        lgb.Dataset(x2, label=y2), \n                        verbose_eval=100, \n                        early_stopping_rounds=200)\n\n# lgb_model = lgb.train(params, \n#                         lgb.Dataset(X_train[fcol], label=y_train),\n#                         verbose_eval=100)\n\nprint(f'Done, time = {time.time() - t}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b4a8b846fc3bae2ab41dd9135f6c928759bbc85"
      },
      "cell_type": "code",
      "source": "def make_predictions(market_obs_df, news_obs_df):\n    \n    print('market_obs_df shape: ' + str(market_obs_df.shape))\n    print('news_obs_df shape: ' + str(news_obs_df.shape))\n    \n    # predict using given model\n    X_test = convert_to_X(market_obs_df, news_obs_df)\n    print('Created X_test with features: ' + str(X_test[fcol].columns))\n    \n    # there is an error:\n    # ValueError: Length of values does not match length of index\n    prediction_values = np.clip(lgb_model.predict(X_test[fcol]), -1, 1)\n    \n    print('finished predictions')\n\n    return prediction_values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3762332a038544f7ed3c9ba5acd5c6fd114b946f"
      },
      "cell_type": "markdown",
      "source": "### Making Predictions\n\nNow the difference between the training and test data would be these two columns,  ``['returnsOpenNextMktres10', 'universe']``. We will be trying to predict ``returnsOpenNextMktres10`` and using that as the ``confidenceValue``."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1fb894dd1216d1dbf6e48d15d336a8fd1847067"
      },
      "cell_type": "code",
      "source": "for (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days(): # Looping over days from start of 2017 to 2019-07-15\n    \n    print('predictions_template_df shape: ' + str(predictions_template_df.shape))\n    # make predictions\n    predictions_template_df['confidenceValue'] = make_predictions(market_obs_df, news_obs_df)\n    \n    # save predictions\n    env.predict(predictions_template_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9c3668a134db935e6a7e266a15a47a7292f537e"
      },
      "cell_type": "markdown",
      "source": "### Export Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02e6a605c9ca804310fbc788b465c226ddfea533"
      },
      "cell_type": "code",
      "source": "env.write_submission_file() # Writes your submission file\nprint('finished!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "### Sources:\n* [Getting Started - DJ Sterling](https://www.kaggle.com/dster/two-sigma-news-official-getting-started-kernel)\n* [Bare bones script - William Cukierski](https://www.kaggle.com/wcukierski/bare-bones-script-loop-with-comments)\n* [Extra data - aaron7sun](https://www.kaggle.com/aaron7sun/stocknews)\n* [Text Preprocessing - Andrew Gelé](https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit)\n* [fake news - SamLloyd](https://www.kaggle.com/sjdlloyd/it-s-fake-news-this-is-top-of-the-leaderboard)\n* [a simple model - Bruno G. do Amaral](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-data)\n* [LGBM Model - the1owl](https://www.kaggle.com/the1owl/my-two-sigma-cents-only)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2db4ea9ce6bed60c3ffa0cf5ff09b0d4efbb589"
      },
      "cell_type": "markdown",
      "source": "# Amateur Hour - Stock Market News\n### Starter Kernel by Magichanics\n*[Gitlab](https://gitlab.com/Magichanics) - [Kaggle](https://www.kaggle.com/magichanics)*\n\nStocks are unpredictable, but can sometimes follow a trend. In this notebook, we will be discovering the correlation between the stocks and the news.\n\nIf there are any things that you would like me to add or remove, feel free to comment down below. I'm mainly doing this to learn and experiment with the data. \n\n**What's new?**\n* October 18th, 2018 - Published kernel\n"
    },
    {
      "metadata": {
        "_uuid": "1e268288023fb7fc142e9f9c1c70f2d04c7cc103"
      },
      "cell_type": "markdown",
      "source": "![title](https://upload.wikimedia.org/wikipedia/commons/8/8d/Wall_Street_sign_banner.jpg)\n\nSource: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Wall_Street_sign_banner.jpg)"
    },
    {
      "metadata": {
        "_uuid": "34cfddef62da37b7f4cc6734f8ece3d2ea4310a4"
      },
      "cell_type": "markdown",
      "source": "### References:\n* [Getting Started - DJ Sterling](https://www.kaggle.com/dster/two-sigma-news-official-getting-started-kernel)\n* [a simple model - Bruno G. do Amaral](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-data)\n* [LGBM Model - the1owl](https://www.kaggle.com/the1owl/my-two-sigma-cents-only)\n* [Headline Processing - Andrew Gel√©](https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74ac86edc14f5f49372114111d24ba7f9c095d97",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os\nfrom itertools import chain\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport gc\n\n# import environment for data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading the data... This could take a minute.\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62bc61f1400a9c5efeb20a460be9ac2066784169"
      },
      "cell_type": "code",
      "source": "sampling = True",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aabe093aa9598bd90bd7bec571df3e8cbdfb8d99"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\n\nif sampling:\n    market_train_df = market_train_df.tail(400_000)\n    news_train_df = news_train_df.tail(1_000_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000) ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07732b5c4888f5de18aa4c424267715dbed5783f"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "                       time   ...    universe\n0 2007-02-01 22:00:00+00:00   ...         1.0\n1 2007-02-01 22:00:00+00:00   ...         0.0\n2 2007-02-01 22:00:00+00:00   ...         1.0\n3 2007-02-01 22:00:00+00:00   ...         1.0\n4 2007-02-01 22:00:00+00:00   ...         1.0\n\n[5 rows x 16 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>assetCode</th>\n      <th>assetName</th>\n      <th>volume</th>\n      <th>close</th>\n      <th>open</th>\n      <th>returnsClosePrevRaw1</th>\n      <th>returnsOpenPrevRaw1</th>\n      <th>returnsClosePrevMktres1</th>\n      <th>returnsOpenPrevMktres1</th>\n      <th>returnsClosePrevRaw10</th>\n      <th>returnsOpenPrevRaw10</th>\n      <th>returnsClosePrevMktres10</th>\n      <th>returnsOpenPrevMktres10</th>\n      <th>returnsOpenNextMktres10</th>\n      <th>universe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2007-02-01 22:00:00+00:00</td>\n      <td>A.N</td>\n      <td>Agilent Technologies Inc</td>\n      <td>2606900.0</td>\n      <td>32.19</td>\n      <td>32.17</td>\n      <td>0.005938</td>\n      <td>0.005312</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.001860</td>\n      <td>0.000622</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.034672</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2007-02-01 22:00:00+00:00</td>\n      <td>AAI.N</td>\n      <td>AirTran Holdings Inc</td>\n      <td>2051600.0</td>\n      <td>11.12</td>\n      <td>11.08</td>\n      <td>0.004517</td>\n      <td>-0.007168</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.078708</td>\n      <td>-0.088066</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.027803</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2007-02-01 22:00:00+00:00</td>\n      <td>AAP.N</td>\n      <td>Advance Auto Parts Inc</td>\n      <td>1164800.0</td>\n      <td>37.51</td>\n      <td>37.99</td>\n      <td>-0.011594</td>\n      <td>0.025648</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.014332</td>\n      <td>0.045405</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.024433</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2007-02-01 22:00:00+00:00</td>\n      <td>AAPL.O</td>\n      <td>Apple Inc</td>\n      <td>23747329.0</td>\n      <td>84.74</td>\n      <td>86.23</td>\n      <td>-0.011548</td>\n      <td>0.016324</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.048613</td>\n      <td>-0.037182</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.007425</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2007-02-01 22:00:00+00:00</td>\n      <td>ABB.N</td>\n      <td>ABB Ltd</td>\n      <td>1208600.0</td>\n      <td>18.02</td>\n      <td>18.01</td>\n      <td>0.011791</td>\n      <td>0.025043</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.012929</td>\n      <td>0.020397</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.017994</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "997773dbe4987fbec07c28775c4343acda7f9521"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "                       time      ...       volumeCounts7D\n0 2007-01-01 04:29:32+00:00      ...                    7\n1 2007-01-01 07:03:35+00:00      ...                    3\n2 2007-01-01 11:29:56+00:00      ...                   17\n3 2007-01-01 12:08:37+00:00      ...                   15\n4 2007-01-01 12:08:37+00:00      ...                    0\n\n[5 rows x 35 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>sourceTimestamp</th>\n      <th>firstCreated</th>\n      <th>sourceId</th>\n      <th>headline</th>\n      <th>urgency</th>\n      <th>takeSequence</th>\n      <th>provider</th>\n      <th>subjects</th>\n      <th>audiences</th>\n      <th>bodySize</th>\n      <th>companyCount</th>\n      <th>headlineTag</th>\n      <th>marketCommentary</th>\n      <th>sentenceCount</th>\n      <th>wordCount</th>\n      <th>assetCodes</th>\n      <th>assetName</th>\n      <th>firstMentionSentence</th>\n      <th>relevance</th>\n      <th>sentimentClass</th>\n      <th>sentimentNegative</th>\n      <th>sentimentNeutral</th>\n      <th>sentimentPositive</th>\n      <th>sentimentWordCount</th>\n      <th>noveltyCount12H</th>\n      <th>noveltyCount24H</th>\n      <th>noveltyCount3D</th>\n      <th>noveltyCount5D</th>\n      <th>noveltyCount7D</th>\n      <th>volumeCounts12H</th>\n      <th>volumeCounts24H</th>\n      <th>volumeCounts3D</th>\n      <th>volumeCounts5D</th>\n      <th>volumeCounts7D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2007-01-01 04:29:32+00:00</td>\n      <td>2007-01-01 04:29:32+00:00</td>\n      <td>2007-01-01 04:29:32+00:00</td>\n      <td>e58c6279551b85cf</td>\n      <td>China's Daqing pumps 43.41 mln tonnes of oil i...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>RTRS</td>\n      <td>{'ENR', 'ASIA', 'CN', 'NGS', 'EMRG', 'RTRS', '...</td>\n      <td>{'Z', 'O', 'OIL'}</td>\n      <td>1438</td>\n      <td>1</td>\n      <td></td>\n      <td>False</td>\n      <td>11</td>\n      <td>275</td>\n      <td>{'0857.HK', '0857.F', '0857.DE', 'PTR.N'}</td>\n      <td>PetroChina Co Ltd</td>\n      <td>6</td>\n      <td>0.235702</td>\n      <td>-1</td>\n      <td>0.500739</td>\n      <td>0.419327</td>\n      <td>0.079934</td>\n      <td>73</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2007-01-01 07:03:35+00:00</td>\n      <td>2007-01-01 07:03:34+00:00</td>\n      <td>2007-01-01 07:03:34+00:00</td>\n      <td>5a31c4327427f63f</td>\n      <td>FEATURE-In kidnapping, finesse works best</td>\n      <td>3</td>\n      <td>1</td>\n      <td>RTRS</td>\n      <td>{'FEA', 'CA', 'LATAM', 'MX', 'INS', 'ASIA', 'I...</td>\n      <td>{'PGE', 'PCO', 'G', 'ESN', 'MD', 'PCU', 'DNP',...</td>\n      <td>4413</td>\n      <td>1</td>\n      <td>FEATURE</td>\n      <td>False</td>\n      <td>55</td>\n      <td>907</td>\n      <td>{'STA.N'}</td>\n      <td>Travelers Companies Inc</td>\n      <td>8</td>\n      <td>0.447214</td>\n      <td>-1</td>\n      <td>0.600082</td>\n      <td>0.345853</td>\n      <td>0.054064</td>\n      <td>62</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2007-01-01 11:29:56+00:00</td>\n      <td>2007-01-01 11:29:56+00:00</td>\n      <td>2007-01-01 11:29:56+00:00</td>\n      <td>1cefd27a40fabdfe</td>\n      <td>PRESS DIGEST - Wall Street Journal - Jan 1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>RTRS</td>\n      <td>{'RET', 'ENR', 'ID', 'BG', 'US', 'PRESS', 'IQ'...</td>\n      <td>{'T', 'DNP', 'PSC', 'U', 'D', 'M', 'RNP', 'PTD...</td>\n      <td>2108</td>\n      <td>2</td>\n      <td>PRESS DIGEST</td>\n      <td>False</td>\n      <td>15</td>\n      <td>388</td>\n      <td>{'WMT.DE', 'WMT.N'}</td>\n      <td>Wal-Mart Stores Inc</td>\n      <td>14</td>\n      <td>0.377964</td>\n      <td>-1</td>\n      <td>0.450049</td>\n      <td>0.295671</td>\n      <td>0.254280</td>\n      <td>67</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>11</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>23768af19dc69992</td>\n      <td>PRESS DIGEST - New York Times - Jan 1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>RTRS</td>\n      <td>{'FUND', 'FIN', 'CA', 'SFWR', 'INS', 'PUB', 'B...</td>\n      <td>{'T', 'DNP', 'PSC', 'U', 'D', 'M', 'RNP', 'PTD...</td>\n      <td>1776</td>\n      <td>6</td>\n      <td>PRESS DIGEST</td>\n      <td>False</td>\n      <td>14</td>\n      <td>325</td>\n      <td>{'GOOG.O', 'GOOG.OQ', 'GOOGa.DE'}</td>\n      <td>Google Inc</td>\n      <td>13</td>\n      <td>0.149071</td>\n      <td>-1</td>\n      <td>0.752917</td>\n      <td>0.162715</td>\n      <td>0.084368</td>\n      <td>83</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>13</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>2007-01-01 12:08:37+00:00</td>\n      <td>23768af19dc69992</td>\n      <td>PRESS DIGEST - New York Times - Jan 1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>RTRS</td>\n      <td>{'FUND', 'FIN', 'CA', 'SFWR', 'INS', 'PUB', 'B...</td>\n      <td>{'T', 'DNP', 'PSC', 'U', 'D', 'M', 'RNP', 'PTD...</td>\n      <td>1776</td>\n      <td>6</td>\n      <td>PRESS DIGEST</td>\n      <td>False</td>\n      <td>14</td>\n      <td>325</td>\n      <td>{'XMSR.O'}</td>\n      <td>XM Satellite Radio Holdings Inc</td>\n      <td>11</td>\n      <td>0.149071</td>\n      <td>-1</td>\n      <td>0.699274</td>\n      <td>0.209360</td>\n      <td>0.091366</td>\n      <td>102</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "cb841bc6ab0199143c83428d5419ba9a28eec13b"
      },
      "cell_type": "markdown",
      "source": "### Information on the Training Data\n* There are no Unknown ``assetName`` in ``news_train_df``, but there are 24 479 rows with Unknown as the ``assetName`` in ``market_train_df``. Merging by ``assetCode`` leaves out Unknown rows, which could be problematic.\n* ``Volume`` has the highest correlation in terms of ``returnsOpenNextMktres10``.\n* Merging by just ``assetCodes`` greatly increases the dataframe (with just 100k rows, it has turned into 10 million rows), although merging by ``assetCodes`` and ``time`` greatly decrease the original dataframe."
    },
    {
      "metadata": {
        "_uuid": "4e79b71ec8de5009a497f5ad71bf80be76e78329"
      },
      "cell_type": "markdown",
      "source": "### Aggregations on News Data\n\nIt helped a lot during the Home Credit competition, and in the next block of code we will be merging the news dataframe with the market dataframe. Instead of having columns with a list of numbers, we will get aggregations for each grouping. The following block creates a dictionary that will be used when merging the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e45bcca68ec56ffb6d1e3d2049aa4970f6897da4"
      },
      "cell_type": "code",
      "source": "news_agg_cols = [f for f in news_train_df.columns if 'novelty' in f or\n                'volume' in f or\n                'sentiment' in f or\n                'bodySize' in f or\n                'Count' in f or\n                'marketCommentary' in f or\n                'relevance' in f]\nnews_agg_dict = {}\nfor col in news_agg_cols:\n    news_agg_dict[col] = ['mean', 'sum', 'max', 'min']\nnews_agg_dict['urgency'] = ['min', 'count']\nnews_agg_dict['takeSequence'] = ['max']",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1185cd336552dfbc3972772b6cc1ad908654b43"
      },
      "cell_type": "markdown",
      "source": "### Joining Market & News Data\n\nThe grouping method that I'll be using is from [bguberfain](https://www.kaggle.com/bguberfain), but I'll also be adding in the headlines column, as well eliminating rows that are not partnered with either the market or news data. One way I would improve this is probably group by time periods rather than exact times given in ``time`` due to the small amount of data that share the same amount of data in terms of the ``time`` column, and possibly making it a bit more efficient.\n\nNOTE: When you run the full dataset, expect it to take a while."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2dad49559a057808e6d2559a6fbac4f964bd6d4"
      },
      "cell_type": "code",
      "source": "# update market dataframe to only contain the specific rows with matching indecies.\ndef check_index(index, indecies):\n    if index in indecies:\n        return True\n    else:\n        return False\n\ndef join_market_news(market_df, news_df, nulls=False):\n\n    print('market_df :' + str(market_df.shape))\n    \n    # Fix asset codes (str -> list)\n    news_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_df['assetCodes']))\n    assetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    # get rid of any rows that will cause null values in one dataframe or the other.\n    if not nulls:\n        \n        # gget new dataframe\n        temp_news_df_expanded = pd.merge(df_assetCodes, news_df[['time', 'assetCodes']], left_on='level_0', right_index=True, suffixes=(['','_old']))\n        \n        # groupby dataframes\n        temp_news_df = temp_news_df_expanded.copy()[['time', 'assetCode']]\n        temp_market_df = market_df.copy()[['time', 'assetCode']]\n        \n        # get indecies on both dataframes\n        temp_news_df['news_index'] = temp_news_df.index.values\n        temp_market_df['market_index'] = temp_market_df.index.values\n        \n        # set multiindex and join the two\n        temp_news_df.set_index(['time', 'assetCode'], inplace=True)\n        \n        # join the two\n        temp_market_df_2 = temp_market_df.join(temp_news_df, on=['time', 'assetCode'])\n        del temp_market_df, temp_news_df\n        \n        # drop nulls in any columns\n        temp_market_df_2 = temp_market_df_2.dropna()\n        print('dataframe relation: ' + str(temp_market_df_2.shape))\n        \n        # get indecies\n        market_valid_indecies = temp_market_df_2['market_index'].tolist()\n        news_valid_indecies = temp_market_df_2['news_index'].tolist()\n        del temp_market_df_2\n            \n        # get index column\n        market_df['market_index'] = market_df.index.values\n        market_df['is_news'] = market_df['market_index'].apply(lambda x: check_index(x, market_valid_indecies))\n        market_df = market_df[market_df.is_news == True]\n        print('new market dataframe: ' + str(market_df.shape))\n        del market_df['market_index'], market_df['is_news']\n    \n    # create dataframe based on groupby\n    news_col = ['time', 'assetCodes', 'headline'] + sorted(list(news_agg_dict.keys()))\n    news_df_expanded = pd.merge(df_assetCodes, news_df[news_col], left_on='level_0', right_index=True, suffixes=(['','_old']))\n    \n    # check if the columns are in the index\n    if news_valid_indecies:\n        news_df_expanded['news_index'] = news_df_expanded.index.values\n        news_df_expanded['is_market'] = news_df_expanded['news_index'].apply(lambda x: check_index(x, news_valid_indecies))\n        news_df_expanded = news_df_expanded[news_df_expanded.is_market == True]\n        print('new news dataframe: ' + str(news_df_expanded.shape))\n        del news_df_expanded['news_index'], news_df_expanded['is_market']\n\n    print('creating grouped data...')\n\n    def news_df_feats(x):\n        if x.name == 'headline':\n            return list(x)\n    \n    # groupby time and assetcode\n    news_df_expanded = news_df_expanded.reset_index()\n    news_groupby = news_df_expanded.groupby(['time', 'assetCode'])\n    \n    # get aggregated df\n    news_df_aggregated = news_groupby.agg(news_agg_dict).apply(np.float32).reset_index()\n    news_df_aggregated.columns = ['_'.join(col).strip() for col in news_df_aggregated.columns.values]\n    \n    # get any important string dataframes\n    news_df_cat = news_groupby.transform(lambda x: news_df_feats(x))['headline'].to_frame()\n    new_news_df = pd.concat([news_df_aggregated, news_df_cat], axis=1)\n    \n    # cleanup\n    del news_df_aggregated\n    del news_df_cat\n    del news_df\n    \n    # rename columns\n    new_news_df.rename(columns={'time_': 'time', 'assetCode_': 'assetCode'}, inplace=True)\n    new_news_df.set_index(['time', 'assetCode'], inplace=True)\n    \n    print('merging data...')\n    \n    # Join with train\n    market_df = market_df.join(new_news_df, on=['time', 'assetCode'])\n\n    # cleanup\n    gc.collect()\n    \n    print('X shape :' + str(market_df.shape))\n    \n    return market_df\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2be1b6e851438405ca972ddf6fdcc6ea8c97cd28"
      },
      "cell_type": "code",
      "source": "%%time\nX_train = join_market_news(market_train_df, news_train_df, nulls=False)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "market_df :(400000, 16)\ndataframe relation: (227, 4)\nnew market dataframe: (182, 18)\nnew news dataframe: (227, 30)\ncreating grouped data...\nmerging data...\nX shape :(182, 104)\nCPU times: user 22 s, sys: 724 ms, total: 22.7 s\nWall time: 22.7 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d38b9ffd46a21e96bc55b142452eb7cec432c68"
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "                             time                        ...                                                                   headline\n3673357 2016-02-22 22:00:00+00:00                        ...                          [Ingram Micro Expands Availability of Acronis ...\n3673877 2016-02-22 22:00:00+00:00                        ...                          [RPT-EXCLUSIVE-Up to 90 million more Takata ai...\n3674049 2016-02-22 22:00:00+00:00                        ...                          [RPT-EXCLUSIVE-Up to 90 million more Takata ai...\n3674110 2016-02-22 22:00:00+00:00                        ...                          [RPT-EXCLUSIVE-Up to 90 million more Takata ai...\n3674877 2016-02-22 22:00:00+00:00                        ...                          [RPT-EXCLUSIVE-Up to 90 million more Takata ai...\n\n[5 rows x 104 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>assetCode</th>\n      <th>assetName</th>\n      <th>volume</th>\n      <th>close</th>\n      <th>open</th>\n      <th>returnsClosePrevRaw1</th>\n      <th>returnsOpenPrevRaw1</th>\n      <th>returnsClosePrevMktres1</th>\n      <th>returnsOpenPrevMktres1</th>\n      <th>returnsClosePrevRaw10</th>\n      <th>returnsOpenPrevRaw10</th>\n      <th>returnsClosePrevMktres10</th>\n      <th>returnsOpenPrevMktres10</th>\n      <th>returnsOpenNextMktres10</th>\n      <th>universe</th>\n      <th>bodySize_mean</th>\n      <th>bodySize_sum</th>\n      <th>bodySize_max</th>\n      <th>bodySize_min</th>\n      <th>companyCount_mean</th>\n      <th>companyCount_sum</th>\n      <th>companyCount_max</th>\n      <th>companyCount_min</th>\n      <th>marketCommentary_mean</th>\n      <th>marketCommentary_sum</th>\n      <th>marketCommentary_max</th>\n      <th>marketCommentary_min</th>\n      <th>sentenceCount_mean</th>\n      <th>sentenceCount_sum</th>\n      <th>sentenceCount_max</th>\n      <th>sentenceCount_min</th>\n      <th>wordCount_mean</th>\n      <th>wordCount_sum</th>\n      <th>wordCount_max</th>\n      <th>wordCount_min</th>\n      <th>relevance_mean</th>\n      <th>relevance_sum</th>\n      <th>relevance_max</th>\n      <th>relevance_min</th>\n      <th>...</th>\n      <th>noveltyCount24H_mean</th>\n      <th>noveltyCount24H_sum</th>\n      <th>noveltyCount24H_max</th>\n      <th>noveltyCount24H_min</th>\n      <th>noveltyCount3D_mean</th>\n      <th>noveltyCount3D_sum</th>\n      <th>noveltyCount3D_max</th>\n      <th>noveltyCount3D_min</th>\n      <th>noveltyCount5D_mean</th>\n      <th>noveltyCount5D_sum</th>\n      <th>noveltyCount5D_max</th>\n      <th>noveltyCount5D_min</th>\n      <th>noveltyCount7D_mean</th>\n      <th>noveltyCount7D_sum</th>\n      <th>noveltyCount7D_max</th>\n      <th>noveltyCount7D_min</th>\n      <th>volumeCounts12H_mean</th>\n      <th>volumeCounts12H_sum</th>\n      <th>volumeCounts12H_max</th>\n      <th>volumeCounts12H_min</th>\n      <th>volumeCounts24H_mean</th>\n      <th>volumeCounts24H_sum</th>\n      <th>volumeCounts24H_max</th>\n      <th>volumeCounts24H_min</th>\n      <th>volumeCounts3D_mean</th>\n      <th>volumeCounts3D_sum</th>\n      <th>volumeCounts3D_max</th>\n      <th>volumeCounts3D_min</th>\n      <th>volumeCounts5D_mean</th>\n      <th>volumeCounts5D_sum</th>\n      <th>volumeCounts5D_max</th>\n      <th>volumeCounts5D_min</th>\n      <th>volumeCounts7D_mean</th>\n      <th>volumeCounts7D_sum</th>\n      <th>volumeCounts7D_max</th>\n      <th>volumeCounts7D_min</th>\n      <th>urgency_min</th>\n      <th>urgency_count</th>\n      <th>takeSequence_max</th>\n      <th>headline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3673357</th>\n      <td>2016-02-22 22:00:00+00:00</td>\n      <td>ALV.N</td>\n      <td>Autoliv Inc</td>\n      <td>436921.0</td>\n      <td>111.96</td>\n      <td>108.99</td>\n      <td>0.029801</td>\n      <td>0.008700</td>\n      <td>0.024366</td>\n      <td>0.005473</td>\n      <td>0.126858</td>\n      <td>0.084695</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.037786</td>\n      <td>0.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>17.0</td>\n      <td>17.0</td>\n      <td>17.0</td>\n      <td>17.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>[Ingram Micro Expands Availability of Acronis ...</td>\n    </tr>\n    <tr>\n      <th>3673877</th>\n      <td>2016-02-22 22:00:00+00:00</td>\n      <td>F.N</td>\n      <td>Ford Motor Co</td>\n      <td>33549732.0</td>\n      <td>12.56</td>\n      <td>12.24</td>\n      <td>0.038017</td>\n      <td>0.004102</td>\n      <td>0.021645</td>\n      <td>0.000050</td>\n      <td>0.096943</td>\n      <td>0.062500</td>\n      <td>0.047356</td>\n      <td>0.055504</td>\n      <td>0.027020</td>\n      <td>1.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>18.0</td>\n      <td>18.0</td>\n      <td>18.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>[RPT-EXCLUSIVE-Up to 90 million more Takata ai...</td>\n    </tr>\n    <tr>\n      <th>3674049</th>\n      <td>2016-02-22 22:00:00+00:00</td>\n      <td>HMC.N</td>\n      <td>Honda Motor Co Ltd</td>\n      <td>610950.0</td>\n      <td>26.11</td>\n      <td>26.12</td>\n      <td>0.006942</td>\n      <td>0.007716</td>\n      <td>-0.004143</td>\n      <td>0.002679</td>\n      <td>0.003845</td>\n      <td>-0.010231</td>\n      <td>-0.017810</td>\n      <td>-0.013049</td>\n      <td>0.024391</td>\n      <td>1.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>0.073771</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>[RPT-EXCLUSIVE-Up to 90 million more Takata ai...</td>\n    </tr>\n    <tr>\n      <th>3674110</th>\n      <td>2016-02-22 22:00:00+00:00</td>\n      <td>IM.N</td>\n      <td>Ingram Micro Inc</td>\n      <td>7136909.0</td>\n      <td>35.92</td>\n      <td>36.17</td>\n      <td>-0.010741</td>\n      <td>0.004443</td>\n      <td>-0.021674</td>\n      <td>-0.002716</td>\n      <td>0.294881</td>\n      <td>0.280807</td>\n      <td>0.220617</td>\n      <td>0.269462</td>\n      <td>-0.058289</td>\n      <td>1.0</td>\n      <td>6256.0</td>\n      <td>6256.0</td>\n      <td>6256.0</td>\n      <td>6256.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>979.0</td>\n      <td>979.0</td>\n      <td>979.0</td>\n      <td>979.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>36.0</td>\n      <td>36.0</td>\n      <td>36.0</td>\n      <td>36.0</td>\n      <td>51.0</td>\n      <td>51.0</td>\n      <td>51.0</td>\n      <td>51.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>[RPT-EXCLUSIVE-Up to 90 million more Takata ai...</td>\n    </tr>\n    <tr>\n      <th>3674877</th>\n      <td>2016-02-22 22:00:00+00:00</td>\n      <td>TM.N</td>\n      <td>Toyota Motor Corp</td>\n      <td>297787.0</td>\n      <td>106.69</td>\n      <td>106.07</td>\n      <td>0.013104</td>\n      <td>0.003690</td>\n      <td>-0.002039</td>\n      <td>-0.001471</td>\n      <td>-0.033518</td>\n      <td>-0.054971</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.036162</td>\n      <td>0.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>8557.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>52.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>0.036886</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>13.0</td>\n      <td>13.0</td>\n      <td>13.0</td>\n      <td>13.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>[RPT-EXCLUSIVE-Up to 90 million more Takata ai...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "957f29e020695f32628882848eecdf392aa9e9e0"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with MultinomialNB"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25c279b7face9bb58eb15a2780a7ec7e9d6b6f8"
      },
      "cell_type": "code",
      "source": "def get_headline(headlines_df):\n    \n    # get headlines as list\n    headlines_lst = []\n    for row in range(0,len(headlines_df.index)):\n        for sentence in headlines_df.iloc[row]:\n            headlines_lst.append(row)\n\n    # split headlines to separate words\n    basicvectorizer = CountVectorizer()\n    headlines_vectorized = basicvectorizer.fit_transform(headlines_lst)\n    \n    print(headlines_vectorized.shape)\n    return headlines_vectorized, basicvectorizer\n\ndef headline_mapping(target, headlines_vectored, headline_vectorizer):\n    \n    # get model (testing with model that isn't )\n    from sklearn.naive_bayes import MultinomialNB\n    headline_model = MultinomialNB()\n    headline_model = headline_model.fit(headlines_vectored, target)\n    \n    # get coefficients\n    basicwords = headline_vectorizer.get_feature_names()\n    basiccoeffs = headline_model.coef_.tolist()[0]\n    coeff_df = pd.DataFrame({'Word' : basicwords, \n                            'Coefficient' : basiccoeffs})\n    \n    # convert dataframe to dictionary of coefficients\n    coefficient_dict = dict(zip(coeff_df.Word, coeff_df.Coefficient))\n\n    return coefficient_dict, coeff_df['Coefficient'].mean()\n\ndef get_coeff_col(headlines_df, coeff_dict, coeff_default):\n    \n    def get_coeff(word_lst):\n        \n        # iter through every word\n        coeff_sum = 0\n        for word in word_lst:\n            if word in coeff_dict:\n                coeff_sum += coeff_dict[word]\n            else:\n                coeff_sum += coeff_default\n        \n        # get average coefficient\n        return coeff_sum / len(word_lst)\n        \n    basicvectorizer = CountVectorizer()\n    \n    # loop through every item\n    headlines_coeff_lst = []\n    for row in range(0,len(headlines_df.index)):\n        for sentence in headlines_df.iloc[row]:\n            headlines_coeff_lst.append(get_coeff(str(sentence).split(' ')))\n    \n    return pd.Series(headlines_coeff_lst)",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b58b721978bceccdf5ae8048defa3064c9babbb"
      },
      "cell_type": "code",
      "source": "coefficient_dict, coefficient_default = headline_mapping(X_train['returnsOpenNextMktres10'],\n                                            *get_headline(X_train['headline']))\n\nX_train['headline_coeff_mean'] = get_coeff_col(X_train['headline'], coefficient_dict, coefficient_default)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2619e87625b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m coefficient_dict, coefficient_default = headline_mapping(X_train['returnsOpenNextMktres10'],\n\u001b[0;32m----> 2\u001b[0;31m                                             *get_headline(X_train['headline']))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline_coeff_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coeff_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficient_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficient_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f1340f86378e>\u001b[0m in \u001b[0;36mget_headline\u001b[0;34m(headlines_df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# split headlines to separate words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbasicvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mheadlines_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasicvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bdad0b0f9d641b5e028548285cd64747dd5a8ff"
      },
      "cell_type": "markdown",
      "source": "### Extra Features ``return``"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d66fbab21e9a05e53dc2a0742546043062c98dc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6bc63a91be4bd007fb95c728aadd14a870358ce"
      },
      "cell_type": "markdown",
      "source": "### Get Time Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83c73101057e4aa8c72784ab3fb3c3b497677fc1"
      },
      "cell_type": "code",
      "source": "# ripped from my previous kernel, NYC Taxi Fare\n\n# first get dates\ndef split_time(df):\n    \n    # convert to string (will find a more efficient way to do this without converting to string)\n    df['time'] = df['time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    \n    # split date_time into categories\n    df['time_day'] = df['time'].str.slice(8,10)\n    df['time_month'] = df['time'].str.slice(5,7)\n    df['time_year'] = df['time'].str.slice(0,4)\n    df['time_hour'] = df['time'].str.slice(11,13)\n    \n    # source: https://www.kaggle.com/nicapotato/taxi-rides-time-analysis-and-oof-lgbm\n    df['temp_time'] = df['time'].str.replace(\" UTC\", \"\")\n    df['temp_time'] = pd.to_datetime(df['temp_time'], format='%Y-%m-%d %H:%M:%S')\n    \n    df['time_day_of_year'] = df.temp_time.dt.dayofyear\n    df['time_week_of_year'] = df.temp_time.dt.weekofyear\n    df[\"time_weekday\"] = df.temp_time.dt.weekday\n    df[\"time_quarter\"] = df.temp_time.dt.quarter\n    \n    del df['temp_time']\n    gc.collect()\n    \n    # convert to non-object columns\n    time_feats = ['time_day', 'time_month', 'time_year', 'time_hour']\n    df[time_feats] = df[time_feats].apply(pd.to_numeric)\n    \n    # determine whether the day is set on a holiday\n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start='2007-01-01', end='2018-09-27').to_pydatetime()\n    df['on_holiday'] = df['time'].str.slice(0,10).apply(lambda x: 1 if x in holidays else 0)\n    \n    # note to self: encode time later on\n    \n    return df\n\nX_train = split_time(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f855dca36dec6cd621f032991abc7beb711022e"
      },
      "cell_type": "code",
      "source": "def get_misc_features(X_df):\n    \n    # Adding daily difference\n    new_col = X_df[\"close\"] - X_df[\"open\"]\n    X_df.insert(loc=6, column=\"daily_diff\", value=new_col)\n    X_df['close_to_open'] =  np.abs(X_df['close'] / X_df['open'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3c97e050f19db860fa531455d45aabea3ec9322"
      },
      "cell_type": "markdown",
      "source": "### Label Encoding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e08ff349608e1ee591566ccdbfcaf78c0180661"
      },
      "cell_type": "code",
      "source": "def group_delete(df, del_features):\n    for f in del_features:\n        del df[f]\n\ndef encoding(df, categorical_feats):\n    df_encoded = pd.get_dummies(df[categorical_feats])\n    df.join(df_encoded, how = 'right')\n    group_delete(df, categorical_feats)\n    print('new shape: ' + str(df.shape))\n    return df\n\ngroup_delete(X_train, ['time', 'sourceId', 'headline', 'assetCodes'])\nX_train = encoding(X_train, [f for f in X_train.columns if X_train[f].dtype == 'object'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cc55c845df528961c590be653077ad6418586e2"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a68828c7def0ec9db579d76c280fb433218acf8b"
      },
      "cell_type": "code",
      "source": "# will use a more efficient way later on\nfcol = [c for c in X_train.columns if c not in ['sourceTimestamp', 'firstCreated', 'returnsOpenNextMktres10', \n                                                'assetName_x', 'universe', 'provider', 'subjects',\n                                               'audiences', 'marketCommentary', 'assetName_y', 'sourceTimestamp'\n                                               'firstCreated']] #<---- added\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07a88fc50ca5727ed3f299ab2a9ffd4b2ae05e33"
      },
      "cell_type": "markdown",
      "source": "### Using LGBM for Modelling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2c866081e0fc66578711db888176ee183f26ec3"
      },
      "cell_type": "code",
      "source": "# prepare x dataframes for modelling/prediction\ndef convert_to_X(market_obs_df, news_obs_df):\n    \n    # this repeats everything that was done previously\n    X_test = join_market_news(market_obs_df, news_obs_df)\n    X_test = aggregations(X_test)\n    X_test['headline_coeff_mean'] = get_coeff_col(X_test['headline'], coefficient_dict, coefficient_default)\n    X_test = split_time(X_test)\n    group_delete(X_test, ['time', 'sourceId', 'headline', 'assetCodes'])\n    X_test = encoding(X_test, ['assetCode', 'headlineTag'])\n    X_test = X_test[[f for f in X_test.columns if 'int' in str(X_test[f].dtype) or 'float' in str(X_test[f].dtype)]]\n    \n    return X_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b70209ab5a7928aa810fb6a9efb1b71fd5698e86"
      },
      "cell_type": "code",
      "source": "y_train = X_train['returnsOpenNextMktres10']\ndel X_train['returnsOpenNextMktres10']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d5b1e3030ee1e87c65689f99260f74f63f23fdb"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\nimport time\n\n# set model and parameters\nparams = {'learning_rate': 0.02, \n          'boosting': 'gbdt', \n          'objective': 'regression', \n          'seed': 2018}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c9c43854bc92e13e6808160edd5725e56790cd6"
      },
      "cell_type": "code",
      "source": "#split data (for cross validation)\nx1, x2, y1, y2 = train_test_split(X_train[fcol], \n                                  y_train, \n                                  test_size=0.25, \n                                  random_state=99)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6929a08a5e902a56e29b8b1fcb49d8f6c7e0fcc2"
      },
      "cell_type": "code",
      "source": "# train\nt = time.time()\nprint('Fitting Up')\n\n# cross validation\nlgb_model = lgb.train(params, \n                        lgb.Dataset(x1, label=y1), \n                        5000, \n                        lgb.Dataset(x2, label=y2), \n                        verbose_eval=100, \n                        early_stopping_rounds=200)\n\n# lgb_model = lgb.train(params, \n#                         lgb.Dataset(X_train[fcol], label=y_train),\n#                         verbose_eval=100)\n\nprint(f'Done, time = {time.time() - t}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b4a8b846fc3bae2ab41dd9135f6c928759bbc85"
      },
      "cell_type": "code",
      "source": "def make_predictions(market_obs_df, news_obs_df):\n    \n    print('market_obs_df shape: ' + str(market_obs_df.shape))\n    print('news_obs_df shape: ' + str(news_obs_df.shape))\n    \n    # predict using given model\n    X_test = convert_to_X(market_obs_df, news_obs_df)\n    print('Created X_test with features: ' + str(X_test[fcol].columns))\n    \n    # there is an error:\n    # ValueError: Length of values does not match length of index\n    prediction_values = np.clip(lgb_model.predict(X_test[fcol]), -1, 1)\n    \n    print('finished predictions')\n\n    return prediction_values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3762332a038544f7ed3c9ba5acd5c6fd114b946f"
      },
      "cell_type": "markdown",
      "source": "### Making Predictions\n\nNow the difference between the training and test data would be these two columns,  ``['returnsOpenNextMktres10', 'universe']``. We will be trying to predict ``returnsOpenNextMktres10`` and using that as the ``confidenceValue``."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1fb894dd1216d1dbf6e48d15d336a8fd1847067"
      },
      "cell_type": "code",
      "source": "for (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days(): # Looping over days from start of 2017 to 2019-07-15\n    \n    print('predictions_template_df shape: ' + str(predictions_template_df.shape))\n    # make predictions\n    predictions_template_df['confidenceValue'] = make_predictions(market_obs_df, news_obs_df)\n    \n    # save predictions\n    env.predict(predictions_template_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9c3668a134db935e6a7e266a15a47a7292f537e"
      },
      "cell_type": "markdown",
      "source": "### Export Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02e6a605c9ca804310fbc788b465c226ddfea533"
      },
      "cell_type": "code",
      "source": "env.write_submission_file() # Writes your submission file\nprint('finished!')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}